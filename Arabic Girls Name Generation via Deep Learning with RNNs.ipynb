{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "dac691ce-536f-48d5-9ddc-b4b4b4c303e1",
      "cell_type": "markdown",
      "source": "# Arabic Girls Name Generation via Deep Learning with RNNs",
      "metadata": {}
    },
    {
      "id": "1442bda2-6ccc-426a-8589-5999eb807015",
      "cell_type": "markdown",
      "source": "## 1- Packages ",
      "metadata": {}
    },
    {
      "id": "5b783178-2c5d-4a87-8fc0-a27af445f524",
      "cell_type": "code",
      "source": "import numpy as np\nfrom utils import *\nimport random\nimport pprint\nimport copy",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "id": "916fd597-4e82-4198-a739-aa5a619a2ea0",
      "cell_type": "markdown",
      "source": "## 2- Dataset",
      "metadata": {}
    },
    {
      "id": "fd0b4612-6ded-4f6d-9550-b6054e91a210",
      "cell_type": "markdown",
      "source": "Load the dataset, convert to lowercase, and compute the total and unique character counts.",
      "metadata": {}
    },
    {
      "id": "fe55883a-6887-43dd-9498-5333a51799e2",
      "cell_type": "code",
      "source": "data = open('Arabic_Girls_names.txt', 'r').read()\ndata= data.lower()\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint('There are %d total characters and %d unique characters.' % (data_size, vocab_size))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "There are 6640 total characters and 26 unique characters.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 46
    },
    {
      "id": "85b1ea20-804b-4059-9a96-184ff3ea8f82",
      "cell_type": "code",
      "source": "chars.append('p')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 47
    },
    {
      "id": "6e974bb0-eb67-4d6a-ac16-393fdc31d5a7",
      "cell_type": "code",
      "source": "chars = sorted(chars)\nprint(chars)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['\\n', '/', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 48
    },
    {
      "id": "06fe8951-f6ae-4f83-a346-0d0201d7fab0",
      "cell_type": "code",
      "source": "char_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }\npp = pprint.PrettyPrinter(indent=4)\npp.pprint(ix_to_char)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{   0: '\\n',\n    1: '/',\n    2: 'a',\n    3: 'b',\n    4: 'c',\n    5: 'd',\n    6: 'e',\n    7: 'f',\n    8: 'g',\n    9: 'h',\n    10: 'i',\n    11: 'j',\n    12: 'k',\n    13: 'l',\n    14: 'm',\n    15: 'n',\n    16: 'o',\n    17: 'p',\n    18: 'q',\n    19: 'r',\n    20: 's',\n    21: 't',\n    22: 'u',\n    23: 'v',\n    24: 'w',\n    25: 'y',\n    26: 'z'}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 49
    },
    {
      "id": "58e14117-3d66-404e-9154-0c43246c33cb",
      "cell_type": "markdown",
      "source": "## 3- the clip function\n\nApply gradient clipping to prevent exploding gradients by restricting their values within a specified range.",
      "metadata": {}
    },
    {
      "id": "01cbbfc0-1b69-4b6c-89a9-83049265b9d5",
      "cell_type": "code",
      "source": "def clip(gradients, maxValue):\n    gradients = copy.deepcopy(gradients)\n    \n    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n   \n    for gradient in [dWax, dWaa, dWya, db, dby]:\n        np.clip(gradient, -maxValue, maxValue, out = gradient)\n    \n    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n    \n    return gradients",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 50
    },
    {
      "id": "7f197aa0-8eba-4d7f-98f7-6461e5a7e795",
      "cell_type": "markdown",
      "source": "## 4- The sample function\nGenerate a sequence of characters from the RNN model by sampling based on output probabilities.",
      "metadata": {}
    },
    {
      "id": "dcce86a2-4e99-429e-891f-fe5be4aa048e",
      "cell_type": "code",
      "source": "def sample(parameters, char_to_ix, seed):\n    \n    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n    vocab_size = by.shape[0]\n    n_a = Waa.shape[1]\n    x = np.zeros((27,1))\n    a_prev = np.zeros((n_a,1))\n    indices = []\n    idx = -1\n    counter = 0\n    newline_character = char_to_ix['\\n']\n    \n    while (idx != newline_character and counter != 50):\n        \n        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n        z = np.dot(Wya, a) + by\n        y = softmax(z)\n        np.random.seed(counter + seed) \n        idx = np.random.choice(range(len(y)), p = y.ravel())\n        indices.append(idx)\n        \n        x = np.zeros((27,1))\n        x[idx] = 1\n        \n        a_prev = a\n        \n        seed += 1\n        counter +=1\n        \n\n    if (counter == 50):\n        indices.append(char_to_ix['\\n'])\n    \n    return indices",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 51
    },
    {
      "id": "06a281ec-825a-44d3-bb88-b5da885a30be",
      "cell_type": "markdown",
      "source": "## 5- the optimize function\nOptimize the RNN model by performing forward and backward passes, clipping gradients, and updating parameters.",
      "metadata": {}
    },
    {
      "id": "d6ddc461-1830-4be1-9ac5-da6a1e5ea10b",
      "cell_type": "code",
      "source": "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n    gradients, a = rnn_backward(X, Y, parameters, cache)\n    gradients = clip(gradients, 5)\n    parameters = update_parameters(parameters, gradients, learning_rate)\n    \n    return loss, gradients, a[len(X)-1]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 52
    },
    {
      "id": "d95c7ff0-e60d-49b5-a8da-e5ee10599dbd",
      "cell_type": "markdown",
      "source": "## 6- Traning The Model",
      "metadata": {}
    },
    {
      "id": "f8fe8628-820d-4ebe-afcd-6a56d354554e",
      "cell_type": "code",
      "source": "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, girls_names = 7, vocab_size = 27, verbose = False):\n    \n    n_x, n_y = vocab_size, vocab_size\n    parameters = initialize_parameters(n_a, n_x, n_y)\n    loss = get_initial_loss(vocab_size, girls_names)\n    names = [x.strip() for x in data_x]\n    np.random.seed(0)\n    np.random.shuffle(names)\n    a_prev = np.zeros((n_a, 1))\n    #######################################################################\n    last_girl_name = \" \"\n    for j in range(num_iterations):\n        idx = j % len(names)\n        single_name = names[idx]\n        single_name_chars = [c for c in single_name]\n        single_name_ix = [char_to_ix[ch] for ch in single_name_chars]\n        X = [None] + single_name_ix\n        ix_newline = char_to_ix['\\n']\n        Y = single_name_ix + [ix_newline] \n        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n        \n        if verbose and j in [0, len(names) -1, len(names)]:\n            print(\"j = \" , j, \"idx = \", idx,) \n        if verbose and j in [0]:\n            print(\"single_name =\", single_name)\n            print(\"single_name_chars\", single_name_chars)\n            print(\"single_name_ix\", single_name_ix)\n            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n        loss = smooth(loss, curr_loss)\n        if j % 2000 == 0:\n            \n            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')    \n            seed = 0\n            for name in range(girls_names):\n                sampled_indices = sample(parameters, char_to_ix, seed)\n                last_girl_name = get_sample(sampled_indices, ix_to_char)\n                print(last_girl_name.replace('\\n', ''))\n                seed += 1  \n      \n            print('\\n')\n        \n    return parameters, last_girl_name",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 53
    },
    {
      "id": "577d9364-3691-4ae1-8959-47dac1bc7f61",
      "cell_type": "code",
      "source": "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, 22001, verbose = True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "j =  0 idx =  0\nsingle_name = maysaa\nsingle_name_chars ['m', 'a', 'y', 's', 'a', 'a']\nsingle_name_ix [14, 2, 25, 20, 2, 2]\n X =  [None, 14, 2, 25, 20, 2, 2] \n Y =        [14, 2, 25, 20, 2, 2, 0] \n\nIteration: 0, Loss: 23.070856\n\nMjzwvsclepndygrpv/riiiut\nJmda\nJzwvsclepndygrpv/riiiut\nMda\nZwvsclepndygrpv/riiiut\nDa\nWvsclepndygrpv/riiiut\n\n\nj =  921 idx =  921\nj =  922 idx =  0\nIteration: 2000, Loss: 18.214813\n\nImuyusah\nHafa\nHuyusah\nIh\nZuyoah\nA\nVusah\n\n\nIteration: 4000, Loss: 15.927020\n\nMaytrifaaheeyah\nLah\nMuwrida\nMad\nZutnah\nFa\nZusaihah\n\n\nIteration: 6000, Loss: 14.876344\n\nMaywor\nMeee\nMuwsnad\nMad\nZutnah\nHaafiya\nYusal\n\n\nIteration: 8000, Loss: 14.305895\n\nMevtouh\nMika\nMussoba\nMad\nZusqah\nHaafiya\nZusamainaobenr\n\n\nIteration: 10000, Loss: 13.932542\n\nNayyah\nMaid\nMussida\nNad\nZusna\nHaadiya\nZoraa\n\n\nIteration: 12000, Loss: 13.624752\n\nNayyosa\nMaid\nMusrida\nNad\nZusl\nIaa\nZora\n\n\nIteration: 14000, Loss: 13.402406\n\nNaztiyah\nMaib\nMussoel\nNab\nZusra\nHaadiya\nZuraa\n\n\nIteration: 16000, Loss: 13.254317\n\nNazriyah\nMhab\nMusrd\nNab\nZurlah\nGaadein\nZuqakdal\n\n\nIteration: 18000, Loss: 13.086758\n\nMayyka\nMaha\nMusoraa\nMahaden\nZurrad\nIbabem\nYusan\n\n\nIteration: 20000, Loss: 13.068037\n\nMaysun\nMeha\nMusoud\nMaeahah\nZurnah\nGaadiya\nZunakhanat\n\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}